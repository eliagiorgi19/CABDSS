---
title: "Assignment text analysis"
author: 
  - "Elia Giorgi/ Statistics and Data Science (blended)"
  - "Robin Vloeberghs / QASS"
  - "Hanqing Zhang / Statistics and Data Science (on campus)"
  - "Student 4 / Programme"
format:
  html:
    toc: true
number-sections: true
warning: false
---

1. Replace '?' so that the result of the next code is 'TRUE'.

We first start (indicated by ^) with 04. Next, we have two options: either select 60 OR (|) select one out of [789] followed by a digit between 0-9. Then we have a literal slash \\/. And finally we select 6 digits \\d{6} to end with ($).

```{r}
library(tidyverse)
str_detect(c("0470/125567"),"^04(60|[789]\\d)\\/\\d{6}$")


```

2. Replace '?' so that the result only retains the '.R', '.qmd' and '.md' files.

To look for multiple patterns at the same time we can use the OR operator "|".
We also know that file types are specified at the end of the names so we will make use of "$".

```{r}
my_files<-c("BigData.docx","BigData.pdf","BigData.R","BigData.qmd","BigData.html","BigData.md","BigData.sas","BigData.py","BigData.zip","BigData.tar")
str_subset(my_files,".R$|.qmd$|.md$")
```

3. Replace '?' so that the result only retains the words having two adjacent vowels. The two adjacent vowels are allowed to be different. Write the code in a general sense, so that it also can be applied to other words.

We look for words that contain any combination of size 2 drawn from [aeiou].

```{r}
words<-c("the","circle","is","very","round","the","triangle","is","usually","not")
str_subset(words,'[aeiou]{2}')
```

4. Replace '?' so that the result only retains the words having two adjacent vowels. The two adjacent vowels are not allowed to be different.

We write out all the options consisting of two identical vowels and look for words with at least one of these patterns.

```{r}
words<-c("keep","the","faith","and","the","root","even","a","misspelled","praayer")
str_subset(words,'aa|ee|ii|oo|uu')
```

5. Does the Kuleuven allow you to scrape its website? Argue.

Answer: 

Yes, it is allowed to scrape the KU Leuven website. However, there some restrictions. The permission for scraping can be found by adding /robots.txt to the URL. When inspecting the website https://www.kuleuven.be/robots.txt we see that everybody (indicated by User-agent: *) can scrape all of the website except for 9 specific directories as indexed by the Disallow statement (e.g. Disallow: /cgi-bin/).


6. Suppose I work for an HR-company and have to read and assess many application letters. Until 2021, I have been responsible for following up on 2000 vacancies. I am worried that there recently has been an increase in AI-use when applicants compose their letters. Therefore, I send you my digital archive of letters that I received before AI (say before 2021) became publicly available (assuming that these letters have been written by humans). I also let an AI tool generate application letters based on those 2000 vacancies for which I had been recruiting. I also provide you with these machine-crafted application letters. It is your job the help me predicting whether the application letters I receive now are genuine (written by humans) or machine-crafted. 

6a. What kind of automated text analysis would you suggest? Argue.

Answer: 
We suggest using supervised machine learning with NLP to build a binary classifier for distinguishing between letters written by humans and those generated by AI. To do the analysis, we should first assign letters before 2021 to "human-written" label and assign those machine-crafted ones to "AI-generated" label. Then train models like logistic regression, decision tree, SVM (Support Vector Machine), etc. to do the classification.
Since we already have clear labels of the letters, supervised learning is the ideal way. If we use unsupervised learning, though we can identify the clusters, we can never confidently assign them as "human-written" or "AI-generated" without extra validation. Additionally, AI-generated letters may be similar to human-written ones in tone, structure and even contents. It will lead to ambiguous or misleading clusters.

6b. What preparatory steps would you suggest in order to make the raw text data be suitable for analysis? (Please integrate in your answer the concepts that were discussed in the course material.)

Answer (max 200 words): 
Before text preprocessing, we should compute global features such as sentence length variance and lexical diversity since they require raw text. Then we start preprocessing:
1. Remove noise
using regular expressions to remove personal details, punctuation, headers, etc.
2. Lowercase
3. Tokenize the text into components
4. Remove stop words
5. Apply stemming and lemmatization
6. Perform part-of-speech tagging to capture grammatical structures
7. Conduct trimming to exclude overly common or rare terms, and apply TF-IDF weighting to highlight distinctive words
8. extract n-grams (e.g. bigrams, trigrams) and do the word embeddings (e.g. Word2Vec) to convert text into numerical features for machines to better accept.
9. Split the processed data into training and test sets to apply cross-validation to better assess model performance.

7. In the 2024 US election campaign, you would like to know what subjects dominated the news in the US. Therefore, you have collected 1000 digital newspaper items about US politics during the month before the elections of 5 November 2024. 

7a. What analysis would you suggest?

Answer: 

7b. Which of the following concepts is important for your analysis? Argue.

* Document-Term-Matrix
* Dictionary approach
