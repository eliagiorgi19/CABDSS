---
title: "Assignment text analysis"
author: 
  - "Elia Giorgi/ Statistics and Data Science (blended)"
  - "Robin Vloeberghs / QASS"
  - "Hanqing Zhang / Statistics and Data Science (on campus)"
  - "Emmanuel Ochuba / QASS"
format:
  html:
    toc: true
number-sections: true
warning: false
---

1. Replace '?' so that the result of the next code is 'TRUE'.

We first start (indicated by ^) with 04. Next, we have two options: either select 60 OR (|) select one out of [789] followed by a digit between 0-9. Then we have a literal slash \\/. And finally we select 6 digits \\d{6} to end with ($).

```{r}
library(tidyverse)
str_detect(c("0470/125567"),"^04(60|[789]\\d)\\/\\d{6}$")


```

2. Replace '?' so that the result only retains the '.R', '.qmd' and '.md' files.

To look for multiple patterns at the same time we can use the OR operator "|".
We also know that file types are specified at the end of the names so we will make use of "$".

```{r}
my_files<-c("BigData.docx","BigData.pdf","BigData.R","BigData.qmd","BigData.html","BigData.md","BigData.sas","BigData.py","BigData.zip","BigData.tar")
str_subset(my_files,".R$|.qmd$|.md$")
```

3. Replace '?' so that the result only retains the words having two adjacent vowels. The two adjacent vowels are allowed to be different. Write the code in a general sense, so that it also can be applied to other words.

We look for words that contain any combination of size 2 drawn from [aeiou].

```{r}
words<-c("the","circle","is","very","round","the","triangle","is","usually","not")
str_subset(words,'[aeiou]{2}')
```

4. Replace '?' so that the result only retains the words having two adjacent vowels. The two adjacent vowels are not allowed to be different.

We write out all the options consisting of two identical vowels and look for words with at least one of these patterns.

```{r}
words<-c("keep","the","faith","and","the","root","even","a","misspelled","praayer")
str_subset(words,'aa|ee|ii|oo|uu')
```

5. Does the Kuleuven allow you to scrape its website? Argue.

Answer: 

Yes, it is allowed to scrape the KU Leuven website. However, there some restrictions. The permission for scraping can be found by adding /robots.txt to the URL. When inspecting the website https://www.kuleuven.be/robots.txt we see that everybody (indicated by User-agent: *) can scrape all of the website except for 9 specific directories as indexed by the Disallow statement (e.g. Disallow: /cgi-bin/).


6. Suppose I work for an HR-company and have to read and assess many application letters. Until 2021, I have been responsible for following up on 2000 vacancies. I am worried that there recently has been an increase in AI-use when applicants compose their letters. Therefore, I send you my digital archive of letters that I received before AI (say before 2021) became publicly available (assuming that these letters have been written by humans). I also let an AI tool generate application letters based on those 2000 vacancies for which I had been recruiting. I also provide you with these machine-crafted application letters. It is your job the help me predicting whether the application letters I receive now are genuine (written by humans) or machine-crafted. 

6a. What kind of automated text analysis would you suggest? Argue.

Answer: 
We suggest using supervised machine learning with NLP to build a binary classifier for distinguishing between letters written by humans and those generated by AI. To do the analysis, we should first assign letters before 2021 to "human-written" label and assign those machine-crafted ones to "AI-generated" label. Then train models like logistic regression, decision tree, SVM (Support Vector Machine), etc. to do the classification.
Since we already have clear labels of the letters, supervised learning is the ideal way. If we use unsupervised learning, though we can identify the clusters, we can never confidently assign them as "human-written" or "AI-generated" without extra validation. Additionally, AI-generated letters may be similar to human-written ones in tone, structure and even contents. It will lead to ambiguous or misleading clusters.

6b. What preparatory steps would you suggest in order to make the raw text data be suitable for analysis? (Please integrate in your answer the concepts that were discussed in the course material.)

Answer (max 200 words): 
Before text preprocessing, we should compute global features such as sentence length variance and lexical diversity since they require raw text. Then we start preprocessing:
1. Remove noise
using regular expressions to remove personal details, punctuation, headers, etc.
2. Lowercase
3. Tokenize the text into components
4. Remove stop words
5. Apply stemming and lemmatization
6. Perform part-of-speech tagging to capture grammatical structures
7. Conduct trimming to exclude overly common or rare terms, and apply TF-IDF weighting to highlight distinctive words
8. extract n-grams (e.g. bigrams, trigrams) and do the word embeddings (e.g. Word2Vec) to convert text into numerical features for machines to better accept.
9. Split the processed data into training and test sets to apply cross-validation to better assess model performance.

7. In the 2024 US election campaign, you would like to know what subjects dominated the news in the US. Therefore, you have collected 1000 digital newspaper items about US politics during the month before the elections of 5 November 2024. 

7a. What analysis would you suggest?

Answer: 

The analysis we propose is Latent Dirichlet Analysis (LDA). LDA is an explorative analysis which allows to uncover the most common topics in the articles. As input, LDA requires the Document-Term-Matrix, which indicates how often specific words occur in a document. LDA will then output the per-document topic proportions and the per-topic word distribution. Especially the latter is of interest as it displays the topics and how they relate to specific words. By interpreting these proportions we can derive what these topics could be. For instance, if a certain topic has a high proportion for the words 'army', 'Russia', 'threat', and 'safety', then we could interpret this topic as 'war'. Since we do not know beforehand how many topics we can reveal we will run the LDA estimation procedure with a varying number of topics and select the best model based on a model metric such as perplexity, while trying to aim for an interpretable number of topics. 

Note that a first sight Visbility Analysis (VA) might seem a good candidate, as it counts how often words (and thus topics) occur in articles. However, VA requires an a priori defined lists with all the words, their synonyms, and even abbreviations. For our case, this is rather problematic since we do not know which topics (and thus all the related words) are present. We could decide on some topics such as for example war or economy but if any topic in the articles is not represented in our list then we will miss this completely and make false inferences. A final argument against VA is that creating this list requires a lot of effort and time, and comes with many researcher's degrees of freedom. However, VA could still be useful as a confirmatory approach, if needed, after we have an idea of the topics present in the articles using LDA. 

7b. Which of the following concepts is important for your analysis? Argue.

* Document-Term-Matrix
* Dictionary approach

The relevant concept for our analysis (Latent Dirichlet Analysis; LDA) is the Document-Term-Matrix, which lists the occurrences of words per document, and is used as input for LDA. The Dictionary approach encompasses analyses such as the visibility analysis or the sentiment analysis, and is not applicable to our case when using LDA. 
